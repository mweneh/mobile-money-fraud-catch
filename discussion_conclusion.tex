\section{Discussion}

This study set out to develop and evaluate a machine learning framework for detecting mobile money fraud under conditions of severe class imbalance. Our findings provide empirical evidence on algorithm performance, the effectiveness of imbalance handling strategies, and the operational trade-offs required for real-world deployment in East African financial ecosystems.

\subsection{Performance Analysis and Algorithm Comparison}

The comparative analysis of five machine learning algorithms revealed distinct performance tiers. \textbf{LightGBM} emerged as the superior model, achieving a PR-AUC of 0.9273 and an optimized F1-score of 0.9067. This performance validates the effectiveness of gradient boosting techniques that utilize leaf-wise tree growth, which allows for capturing complex, non-linear fraud patterns more efficiently than depth-wise approaches.

\textbf{Random Forest} also demonstrated robust performance (PR-AUC 0.9020), confirming the stability of bagging ensembles. However, it required a significantly lower decision threshold (0.70) to achieve comparable recall to LightGBM, suggesting slightly less discrimination power at high-confidence intervals.

In contrast, \textbf{XGBoost} exhibited unexpectedly moderate performance (PR-AUC 0.5469) when using class weighting alone. While it achieved a high ROC-AUC (0.9988), its poor precision-recall balance highlights the misleading nature of ROC-AUC in imbalanced datasets—a finding that aligns with recent critiques in fraud detection literature \citep{Sariat, Zhao2024Improved}. The optimization landscape for XGBoost appears to be more sensitive to hyperparameter configurations in this specific domain compared to LightGBM.

\textbf{Linear models} (Logistic Regression and LinearSVC) failed to provide discriminative power sufficient for operational use, achieving PR-AUC scores below 0.60. Their inability to model complex, non-linear interactions between Transaction Amount, Time, and User History confirms that mobile money fraud patterns are inherently non-linear and cannot be separated by simple hyperplanes.

\subsection{Imbalance Handling: Class Weighting vs. SMOTE}

A critical contribution of this study is the direct comparison between Cost-Sensitive Learning (Class Weighting) and Synthetic Oversampling (SMOTE).

\textbf{SMOTE Integration Trends:}
\begin{itemize}
    \item \textbf{Benefit to XGBoost:} SMOTE significantly boosted XGBoost's performance, raising its PR-AUC from 0.5469 to 0.9411. This suggests that XGBoost struggles to learn minority class boundaries from sparse data alone and benefits heavily from the denser decision surface created by synthetic interpolation.
    \item \textbf{LightGBM Robustness:} Interestingly, LightGBM performed best with Class Weighting (F1 0.9067) rather than SMOTE (F1 0.8800). This indicates that for highly efficient algorithms like LightGBM, preserving the original data distribution is more valuable than introducing synthetic noise.
\end{itemize}

\textbf{Computational Efficiency Trade-off:}
While SMOTE offers predictive gains for certain algorithms, it introduces a significant computational cost. Applying SMOTE increased the training set size from 76,529 to 152,750 samples (a 100\% increase). In a production environment processing millions of daily transactions, doubling the training data volume linearly increases memory usage and Training time. 

In contrast, \textbf{Class Weighting} incurs negligible computational overhead as it modifies the loss function gradient without altering the dataset size. Given that LightGBM with Class Weighting achieved the highest overall F1-score without the need for data augmentation, it represents the most computationally efficient strategy for scalable mobile money fraud detection.

\subsection{The Criticality of Threshold Optimization}

Our results underscore that default classification thresholds (0.5) are suboptimal for fraud detection. For LightGBM, shifting the threshold from 0.5 to 0.9964 resulted in a \textbf{69\% improvement in F1-score}.

This extreme optimal threshold (near 1.0) reveals a key insight: the model is highly confident in its fraud predictions. By moving the decision boundary to 0.9964, we effectively filtered out a vast "grey zone" of false positives while retaining 87\% of true fraud cases. In operational terms, this translates to a massive reduction in analyst workload. At the default threshold, the system would flag $\sim$160 false positives for every 100 true frauds; at the optimized threshold, this drops to nearly zero, allowing fraud teams to focus purely on high-probability investigations.

\subsection{Operational Feasibility and Feature Importance}

The feature importance analysis identified \textbf{Transaction Amount} and \textbf{Value} as the dominant predictors. This aligns with financial intuition—fraudsters typically aim to maximize extraction value. However, the strong contribution of temporal features (Hour, Day) suggests that fraud attempts follow distinct behavioral cycles (e.g., late-night attacks) that differ from legitimate user patterns.

From a deployment perspective, the LightGBM model is highly feasible. Its ability to process batches of transactions with millisecond-level latency (proven by the prototype `app.py`) fits within the strict SLAs of mobile money gateways. The reliance on purely transactional features (Amount, Time, Account Stats) ensures that the model can run in real-time inference engines without waiting for complex external data lookups.

\subsection{Limitations}
This study was limited to a specific dataset from the East African region. While representative of emerging markets, fraud patterns may vary in other jurisdictions due to different regulatory controls and user behaviors. Additionally, the study utilized static historical data; future work must address \textbf{concept drift}, where fraud patterns evolve in response to detection measures, necessitating adaptive online learning frameworks.

\section{Conclusion}

This study demonstrates that machine learning can effectively secure mobile money ecosystems against fraud, provided that the modeling approach is tailored to the specific challenges of the domain. We successfully developed a fraud detection framework using the CRISP-DM methodology, benchmarking five algorithms on real-world transaction data.

\textbf{Key Contributions:}
\begin{enumerate}
    \item \textbf{Algorithmic Benchmark:} We identified LightGBM as the optimal classifier for this domain, outperforming industry standards like Random Forest and XGBoost in both accuracy and efficiency.
    \item \textbf{Imbalance Strategy:} We provided empirical evidence that Cost-Sensitive Learning (Class Weighting) is superior to SMOTE for high-performance gradient boosting models, offering better scalabilty for large datasets.
    \item \textbf{Operational Readiness:} We proved that threshold optimization is not optional but a critical step that can double model effectiveness, converting a theoretical model into a viable business tool.
\end{enumerate}

As mobile money continues to drive financial inclusion globally, robust automated fraud detection will remain a cornerstone of trust. The framework presented in this paper offers a scalable, data-driven path forward for financial service providers to protect their users and their platforms from evolving financial crimes.
